{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch5 Object Detection and Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (p.97) Basics of Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (p.98) Object Detection vs. Object Recognition\n",
    "- Object recognition is the method of identifying an object within an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (p.99) Template Matching\n",
    "- Template matching is a brute-force algorithm or a simple mechanism to extract.\n",
    "- OpenCV has a matchTemplate() function to perform template matching.\n",
    "- Typical object detection procedure: image sequence >> image preprocessing >> object detection >> object segmentation >> feature extraction >> matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical Code\n",
    "\n",
    "#### Lines 1 and 2: Import the OpenCV and NumPy libraries.\n",
    "1 import cv2\n",
    "\n",
    "2 import numpy as np\n",
    "\n",
    "3\n",
    "#### Lines 4 through 8: Load the image that needs to be searched for and convert it to grayscale.\n",
    "4 # Load input image and convert to grayscale\n",
    "\n",
    "5 image = cv2.imread('./images/inputImage.jpg')\n",
    "\n",
    "6 cv2.imshow('Where is this image?', image)\n",
    "\n",
    "7 cv2.waitKey(0)\n",
    "\n",
    "8 gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "9\n",
    "#### Line 10: Loads the bigger image in which the input image needs to be searched for.\n",
    "10 # Load Bigger image\n",
    "\n",
    "11 bigger_image = cv2.imread('./images/searchImage.jpg',0)\n",
    "\n",
    "12\n",
    "#### Lines 13 and 14: cv2.matchTemplate() returns a correlation map, essentially a grayscale image. This image has each pixel that denotes the extent to which its neighborhood matches with the template. The minMaxLoc function returns the max and min intensity values as an array that includes the location of these intensities.\n",
    "13 result = cv2.matchTemplate(gray, template, cv2.TM_CCOEFF)\n",
    "\n",
    "14 min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "\n",
    "15\n",
    "\n",
    "16 #Create Bounding Box\n",
    "\n",
    "17 top_left = max_loc\n",
    "\n",
    "18 bottom_right = (top_left[0] + 50, top_left[1] + 50)\n",
    "\n",
    "19 cv2.rectangle(image, top_left, bottom_right, (0,0,255), 5)\n",
    "\n",
    "20\n",
    "\n",
    "21 cv2.imshow('Where is input image?', image)\n",
    "\n",
    "22 cv2.waitKey(0)\n",
    "\n",
    "23 cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (p.102) Challenges with Template Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (p.102) Understanding Image \"Features\"\n",
    "- Image featureâ€“driven object detection and recognition\n",
    "- Interesting and uninteresting points:\n",
    "Is repeatable? Is salient/distinctive/unique? Is compact in number? Is local?\n",
    "- Types of image features: region, edge, coner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (P.105) Feature Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (p.105) Image Coners as Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (p.106) Harris Coner Algorithm\n",
    "In OpenCV, the cv2.cornerHarris() function is used to achieve the corner detection.\n",
    "\n",
    "#### cv2.cornerHarris(image, blockSize, ksize, k)\n",
    "This function takes four arguments.\n",
    "- img is the image to be analyzed; it must be in grayscale and with float32 values.\n",
    "- blockSize is the size of the window considered for the corner detection.\n",
    "- ksize is a parameter for the derivative of Sobel.\n",
    "- k is a free parameter for the Harris equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "img = cv2.imread('./images/arrow.jpg', 0)\n",
    "img = np.float32(img)\n",
    "corners = cv2.cornerHarris(img,2,3,0.04)\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.imshow(corners ,cmap = 'jet')\n",
    "plt.title('Harris Corner Detection')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "img2 = cv2.imread('./images/arrow.jpg')\n",
    "corners2 = cv2.dilate(corners, None, iterations=3)\n",
    "img2[corners2>0.01*corners2.max()] = [255,0,0]\n",
    "plt.subplot(2,1,2)\n",
    "plt.imshow(img2,cmap = 'gray')\n",
    "plt.xticks([]),\n",
    "plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (p.108) Feature Tracking and Matching Flow\n",
    "#### Standard flow for feature extraction and matching:\n",
    "- Create feature detector: ex) SIEF, SURF, FAST, BRIEF, ORB detectors\n",
    "- Input an image into the detector\n",
    "- Extract key points\n",
    "- Draw key points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (p.109) Scale Invariant Feature Transform (SIFT)\n",
    "www.inf.fu-berlin.de/lehre/SS09/CV/uebungen/uebung09/SIFT.pdf\n",
    "\n",
    "http://www.gisdeveloper.co.kr/?p=6779"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('./images/home.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "kp = sift.detect(gray,None)\n",
    " \n",
    "sift_img=cv2.drawKeypoints(gray,kp,None,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    " \n",
    "cv2.imshow('Original', img)\n",
    "cv2.imshow('SIFT Feature Transform', sift_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (p.112) Speeded-Up Robust Features\n",
    "\n",
    "www.vision.ee.ethz.ch/~surf/eccv06.pdf\n",
    "\n",
    "https://docs.opencv.org/master/df/dd2/tutorial_py_surf_intro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keypoints Detected:  48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread('./images/fly.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Create SURF Feature Detector object\n",
    "surf = cv2.xfeatures2d.SURF_create(400)\n",
    "\n",
    "# Only features, whose hessian is larger than hessianThreshold are retained by the detector\n",
    "surf.setHessianThreshold(50000)\n",
    "keypoints, descriptors = surf.detectAndCompute(gray, None)\n",
    "print (\"Number of keypoints Detected: \", len(keypoints))\n",
    "\n",
    "# Draw rich key points on input image\n",
    "image_surf = cv2.drawKeypoints(gray, keypoints, None,(255,0,0),4)\n",
    "\n",
    "cv2.imshow('Original', image)\n",
    "cv2.imshow('Feature Method - SURF', image_surf)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (p.113) Features from Accelerated Segment Test (FAST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (p.114) Binary Robust Independent Elementary Features (BRIEF)\n",
    "http://cvlabwww.epfl.ch/~lepetit/papers/calonder_pami11.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (p.116) Oriented FAST and Rotated BRIEF\n",
    "http://www.willowgarage.com/sites/default/files/orb_final.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
